import React, { useEffect, useRef, useState } from 'react';
import { useNavigate } from 'react-router-dom';
import { useLocation } from 'react-router-dom';
import io from 'socket.io-client';
import '../CSS/TakeAttendenceCamera.css';

function TakeAttendenceCamera() {
    const [socket, setSocket] = useState(null);
    const [running, setRunning] = useState(false);
    const [attendanceList, setAttendanceList] = useState([]);
    const [videoFrame, setVideoFrame] = useState(null);
    const videoRef = useRef(null);
    const videoStreamRef = useRef(null);
    const captureIntervalRef = useRef(null);
    const navigate = useNavigate();
    const location = useLocation();
    const runningRef = useRef(false);

    const { selectedOptionsIntake, selectedOptionsCourse } = location.state || {};


    console.log('Selected Intake:', selectedOptionsIntake);
    console.log('Selected Course:', selectedOptionsCourse);

    
    // Connect to Socket.IO when component mounts
    // In TakeAttendenceCamera.jsx
    useEffect(() => {
        // Create the socket connection
        try {
            const newSocket = io('http://localhost:5000', {
                reconnection: true,
                reconnectionAttempts: 5,
                reconnectionDelay: 1000,
                timeout: 10000
            });
            
            newSocket.busy = false; // Add busy state management
            console.log('Socket connection initialized');
            
            newSocket.on('connect', () => {
                console.log('Connected to server');
                setSocket(newSocket);
            });
            
            newSocket.on('connect_error', (error) => {
                console.error('Socket connection error:', error);
                alert('Error connecting to server. Please refresh the page.');
            });
            
            newSocket.on('video_frame', (data) => {
                // Update video frame with processed image from server
                if (data && data.frame) {
                    console.log('Received processed frame from server');
                    setVideoFrame(`data:image/jpeg;base64,${data.frame}`);
                    // Mark socket as ready for next frame
                    newSocket.busy = false;
                }
            });
            
            newSocket.on('recognition_event', (data) => {
                if (data.type === 'recognition') {
                    const recognizedName = data.name;
                    console.log(`Recognized: ${recognizedName} (similarity: ${data.similarity})`);
                    
                    // Add to attendance list if not already present
                    const currentTime = new Date().toLocaleTimeString();
                    const [studentName, studentId] = recognizedName.split('_');
                    
                    setAttendanceList(prevList => {
                        // Check if student is already in the list
                        if (!prevList.some(student => student.id === studentId)) {
                            console.log(`Adding student to attendance: ${studentName} (${studentId})`);
                            return [...prevList, {
                                id: studentId,
                                name: studentName,
                                time: currentTime
                            }];
                        }
                        return prevList;
                    });
                }
            });
            
            return () => {
                // Clean up on component unmount
                if (captureIntervalRef.current) {
                    clearInterval(captureIntervalRef.current);
                }
                
                if (videoStreamRef.current) {
                    const tracks = videoStreamRef.current.getTracks();
                    tracks.forEach(track => track.stop());
                }
                
                newSocket.disconnect();
            };
        } catch (err) {
            console.error('Failed to initialize socket:', err);
        }
    }, []);
    
    // Start the camera and begin sending frames to server
    // In the startDetection function
// Modify the startDetection function
    const startDetection = async () => {
        if (!selectedOptionsIntake || !selectedOptionsCourse) {
            alert('Please go back and select Intake and Course again.');
            return;
        }
        
        try {
            // Stop any existing stream first
            if (videoStreamRef.current) {
                const tracks = videoStreamRef.current.getTracks();
                tracks.forEach(track => track.stop());
            }
            
            // Get new video stream
            const stream = await navigator.mediaDevices.getUserMedia({ 
                video: { 
                    width: { ideal: 640 },
                    height: { ideal: 480 }
                } 
            });
            
            videoStreamRef.current = stream;
            
            // Make sure videoRef exists
            if (!videoRef.current) {
                console.error("Video element reference is null");
                alert("Video element not found. Please refresh the page.");
                return;
            }
            
            // Set up the video element
            videoRef.current.srcObject = stream;
            
            // Wait for video to be fully loaded before proceeding
            await new Promise((resolve) => {
                const videoElement = videoRef.current;
                
                // Handle case where video is already loaded
                if (videoElement.readyState >= 3) {
                    resolve();
                    return;
                }
                
                // Set up event listener for when video is ready
                const handleVideoReady = () => {
                    console.log("Video metadata loaded, dimensions:", videoElement.videoWidth, "x", videoElement.videoHeight);
                    videoElement.removeEventListener('loadeddata', handleVideoReady);
                    resolve();
                };
                
                videoElement.addEventListener('loadeddata', handleVideoReady);
            });
            
            // Start playing the video
            await videoRef.current.play();
            console.log('Video is now playing with dimensions:', 
                        videoRef.current.videoWidth, 'x', videoRef.current.videoHeight);
            
            // Now check if socket exists before using it
            if (socket) {
                console.log('Sending start_recognition to server');
                socket.emit('start_recognition', {
                    intake: selectedOptionsIntake,
                    course: selectedOptionsCourse
                }, (response) => {
                    console.log('Received start_recognition response:', response);
                    if (response && response.status === 'success') {
                        setRunning(true);
                        runningRef.current = true;
                        console.log('Recognition started successfully, setting running=true');
                        
                        // Make sure we're not creating multiple intervals
                        if (captureIntervalRef.current) {
                            clearInterval(captureIntervalRef.current);
                        }
                        
                        // Add a small delay before starting capture
                        setTimeout(() => {
                            // Double-check video is still valid before starting interval
                            if (videoRef.current && videoRef.current.videoWidth > 0) {
                                captureIntervalRef.current = setInterval(() => {
                                    if (runningRef.current) {
                                        captureAndSendFrame();
                                    }
                                }, 3000);
                            } else {
                                console.error("Video not valid after ready state");
                                alert("Could not initialize video properly. Please try again.");
                                stopDetection();
                            }
                        }, 500);
                    } else {
                        alert('Failed to start recognition: ' + (response ? response.message : 'Unknown error'));
                    }
                });
            } else {
                alert('Socket connection not established. Please refresh the page.');
            }
        } catch (error) {
            console.error('Error accessing camera:', error);
            alert('Cannot access camera. Please check permissions: ' + error.message);
        }
    };
    
    // Capture frame from video and send to server
    // Capture frame from video and send to server
    // Modify the captureAndSendFrame function to downscale images
    // Capture frame from video and send to server
    const captureAndSendFrame = () => {
        if (!runningRef.current || !socket || socket.busy) {
            return;
        }
    
        const videoElement = videoRef.current;
        if (!videoElement || videoElement.paused || videoElement.ended || videoElement.readyState < 3) {
            return;
        }
    
        try {
            const canvas = document.createElement('canvas');
            const context = canvas.getContext('2d');
            
            // Use a smaller size for better performance
            const maxWidth = 240; // Adjusted for performance
            const scaleFactor = maxWidth / videoElement.videoWidth;
            const height = Math.floor(videoElement.videoHeight * scaleFactor);
            
            canvas.width = maxWidth;
            canvas.height = height;
            
            context.drawImage(videoElement, 0, 0, canvas.width, canvas.height);
            
            const imageData = canvas.toDataURL('image/jpeg', 0.5);
            
            socket.busy = true;
            socket.emit('process_frame', { image: imageData });
        } catch (err) {
            console.error('Error capturing frame:', err);
            socket.busy = false; // Reset busy state on error
        }
    };

    
    
    // Stop detection
    const stopDetection = () => {
        if (!running) return;
        
        setRunning(false);
        runningRef.current = false;
        
        if (captureIntervalRef.current) {
            clearInterval(captureIntervalRef.current);
            captureIntervalRef.current = null;
        }
        
        if (socket) {
            socket.emit('stop_recognition', (response) => {
                console.log('Recognition stopped:', response);
            });
        }
        
        if (videoStreamRef.current) {
            const tracks = videoStreamRef.current.getTracks();
            tracks.forEach(track => track.stop());
            videoStreamRef.current = null;
        }
        
        // Clear the video frame
        setVideoFrame(null);
    };
    
    const goBack = () => {
        // Make sure to stop everything before navigating away
        stopDetection();
        navigate('/attendencepage');
    };

    return (
        <div className="split-container">
            <div className="left-pane">
                <div className="camera_canvas">
                    {/* Keep video element always in DOM but hide it when showing processed frame */}
                    <video 
                        ref={videoRef} 
                        width="600" 
                        height="480" 
                        autoPlay 
                        style={{ 
                            border: '2px solid #333',
                            display: running && videoFrame ? 'none' : 'block' 
                        }} 
                    />
                    
                    {/* Show processed image when available */}
                    {running && videoFrame && (
                        <img 
                            src={videoFrame} 
                            alt="Processed video feed"
                            width="600" 
                            height="480" 
                            style={{ border: '2px solid #333' }} 
                        />
                    )}
                </div>
                <div className="control-buttons">
                    <button type="button" onClick={startDetection} disabled={running}>
                        Start Recognition
                    </button>
                    <button type="button" onClick={stopDetection} disabled={!running}>
                        Stop Recognition
                    </button>
                    <button type="button" onClick={goBack}>
                        Back
                    </button>
                </div>
            </div>
            <div className="right-pane">
                <h2>Attendance List</h2>
                <p>Total Students: {attendanceList.length}</p>
                <table className="attendence_table">
                    <thead>
                        <tr>
                            <th>Student ID</th>
                            <th>Name</th>
                            <th>Time</th>
                        </tr>
                    </thead>
                    <tbody>
                        {attendanceList.length > 0 ? (
                            attendanceList.map((student) => (
                                <tr key={student.id}>
                                    <td>{student.id}</td>
                                    <td>{student.name}</td>
                                    <td>{student.time}</td>
                                </tr>
                            ))
                        ) : (
                            <tr>
                                <td colSpan="3" style={{ textAlign: 'center' }}>No students detected yet</td>
                            </tr>
                        )}
                    </tbody>
                </table>  
            </div>
        </div>
    );
}

export default TakeAttendenceCamera;



import eventlet
eventlet.monkey_patch()

import os
import pandas as pd
import base64
import numpy as np
import cv2
import io
import time
from flask import Flask, request, jsonify
from flask_cors import CORS
from flask_socketio import SocketIO
from datetime import date
from deepface import DeepFace
from mtcnn import MTCNN
from PIL import Image
from numpy.linalg import norm

detector = MTCNN()

app = Flask(__name__)
CORS(app)  # Allow frontend to access backend
socketio = SocketIO(app, cors_allowed_origins="*", async_mode='eventlet')

# Add WebSocket functionality
@socketio.on('connect')
def handle_connect():
    print('Client connected')

@socketio.on('disconnect')
def handle_disconnect():
    print('Client disconnected')


@app.route('/register', methods=['POST'])
def register():
    data = request.get_json()
    name = data['name']
    studentid = data['studentid']
    intake = data['intake']
    course = data['course']
    today = date.today().strftime('%Y-%m-%d')

    # First create student embeddings
    embedding_result = create_student_embeddings(data)
    if "error" in embedding_result:
        return jsonify({"message": embedding_result["error"]}), 400

    folder_path = f"../Students/{intake}/{course}"
    os.makedirs(folder_path, exist_ok=True)

    file_name = f"{intake} {course}.xlsx"
    file_path = os.path.join(folder_path, file_name)

    new_row = pd.DataFrame([{
        "Name": name,
        "StudentID": studentid,
        "Intake": intake,
        "Course": course,
        "Date": today
    }])

    if os.path.exists(file_path):
        df = pd.read_excel(file_path)

        # 🔒 Convert all StudentID values to string and strip spaces
        df['StudentID'] = df['StudentID'].astype(str).str.strip()
        cleaned_studentid = str(studentid).strip()

        # ✅ Now check for duplicates
        if cleaned_studentid in df['StudentID'].values:
            print(f"Duplicate found for ID: {cleaned_studentid}")
            return jsonify({"message": "Student ID already registered!"}), 400

        df = pd.concat([df, new_row], ignore_index=True)

    else:
        df = new_row

    df.to_excel(file_path, index=False)
    print(f"Registered: {studentid} - {name}")
    return jsonify({"message": "Student registered successfully!"})


@app.route('/remove-student', methods=['POST'])
def remove_student():
    data = request.get_json()
    name = data['name'].strip().lower()
    studentid = str(data['studentid']).strip()
    intake = data['intake']
    course = data['course']

    file_path = os.path.join(f"../Students/{intake}/{course}", f"{intake} {course}.xlsx")

    if not os.path.exists(file_path):
        return jsonify({"message": "File not found for given intake and course."}), 404

    df = pd.read_excel(file_path)

    # Normalize for case-insensitive comparison
    df['StudentID'] = df['StudentID'].astype(str).str.strip()
    df['Name'] = df['Name'].astype(str).str.strip().str.lower()

    # Check if row exists
    match = (df['StudentID'] == studentid) & (df['Name'] == name)

    if match.sum() == 0:
        return jsonify({"message": "No matching student found."}), 404

    # Remove matching rows
    df = df[~match]

    # Save the updated file
    df.to_excel(file_path, index=False)

    # Also remove the embedding file if it exists
    embedding_path = os.path.join(f"Students/{intake}/{course}", f"{name}_{studentid}.npy")
    if os.path.exists(embedding_path):
        os.remove(embedding_path)
        print(f"Removed embedding file: {embedding_path}")

    return jsonify({"message": "Student removed successfully!"})


def create_student_embeddings(data):
    name = data['name']
    studentid = str(data['studentid'])
    intake = data['intake']
    course = data['course']
    images = data['images']  # 30 base64 images

    # Folder path
    folder_path = os.path.join("Students", intake, course)
    os.makedirs(folder_path, exist_ok=True)

    embeddings = []
    valid_images = 0

    for i, img_base64 in enumerate(images):
        try:
            # Skip the data URL prefix if present
            if ',' in img_base64:
                img_data = base64.b64decode(img_base64.split(',')[1])
            else:
                img_data = base64.b64decode(img_base64)

            nparr = np.frombuffer(img_data, np.uint8)
            img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

            if img is None or img.size == 0:
                print(f"Image {i} is invalid")
                continue

            # Detect face
            results = detector.detect_faces(img)
            if not results:
                print(f"No face detected in image {i}")
                continue

            x, y, w, h = results[0]['box']
            x, y = max(0, x), max(0, y)
            cropped = img[y:y + h, x:x + w]

            if cropped.size == 0:
                print(f"Cropped face in image {i} is empty")
                continue

            # Resize and preprocess
            cropped = cv2.resize(cropped, (112, 112))

            # Get embedding
            try:
                emb = DeepFace.represent(cropped, model_name='ArcFace', enforce_detection=False)[0]['embedding']
                embeddings.append(emb)
                valid_images += 1
                print(f"Processed image {i} successfully")
            except Exception as e:
                print(f"Embedding error for image {i}: {e}")

        except Exception as e:
            print(f"Error processing image {i}: {e}")

    # Save mean embedding
    if embeddings:
        mean_embedding = np.mean(embeddings, axis=0)
        filename = f"{name}_{studentid}.npy"
        np.save(os.path.join(folder_path, filename), mean_embedding)
        print(f"Created embedding from {valid_images} valid images")
        return {"success": f"Embedding created from {valid_images} images"}
    else:
        print("No valid faces found in the images")
        return {"error": "No valid faces found in the images"}


loaded_embeddings = {}


@app.route('/load-embeddings', methods=['POST'])
def load_student_embeddings():
    global loaded_embeddings
    data = request.get_json()
    intakes = data['intakes']
    courses = data['courses']

    base_path = 'Students'
    loaded_embeddings.clear()
    count = 0

    for intake in intakes:
        for course in courses:
            path = os.path.join(base_path, intake, course)
            if os.path.exists(path):
                for file in os.listdir(path):
                    if file.endswith('.npy'):
                        name = file.replace('.npy', '')
                        try:
                            embedding = np.load(os.path.join(path, file))
                            loaded_embeddings[name] = embedding
                            count += 1
                        except Exception as e:
                            print(f"Error loading embedding {file}: {e}")

    print(f"Loaded {count} embeddings")
    return jsonify({
        "message": f"Loaded {count} student embeddings successfully.",
        "count": count
    })


def cosine_similarity(a, b):
    return np.dot(a, b) / (norm(a) * norm(b))


@app.route('/recognize-face', methods=['POST'])
def recognize_face():
    global loaded_embeddings
    start = time.time()

    # Check if embeddings are loaded
    if not loaded_embeddings:
        return jsonify({"message": "No embeddings loaded. Please load embeddings first."}), 400

    data = request.get_json()

    try:
        # Process the incoming image
        img_data = data['image'].split(',')[1]
        img_bytes = base64.b64decode(img_data)
        img = Image.open(io.BytesIO(img_bytes)).convert('RGB')
        img_np = np.array(img)

        # Detect faces in the image
        faces = detector.detect_faces(img_np)
        if not faces:
            print("⚠️ No face detected")
            return jsonify({"message": "No face detected"}), 400

        # Process the detected face
        face = faces[0]
        x, y, w, h = face["box"]
        print(f"🔍 Face detected at: {x},{y},{w},{h}")

        # Crop and preprocess the face
        cropped_face = img_np[y:y + h, x:x + w]
        cropped_face = cv2.resize(cropped_face, (112, 112))

        # Generate embedding for the detected face
        try:
            new_embedding = DeepFace.represent(cropped_face, model_name='ArcFace', enforce_detection=False)[0][
                'embedding']
        except Exception as e:
            print(f"❌ Embedding error: {e}")
            return jsonify({"message": "Failed to generate face embedding"}), 400

        # Find the best match
        best_match = None
        best_similarity = 0

        for name, saved_emb in loaded_embeddings.items():
            similarity = cosine_similarity(new_embedding, saved_emb)
            print(f"Similarity with {name}: {similarity:.4f}")

            if similarity > best_similarity:
                best_similarity = similarity
                best_match = name

        # Check if the best match exceeds the threshold
        if best_similarity > 0.75:
            print(f"✅ Recognized {best_match} (similarity: {best_similarity:.4f})")
            elapsed = time.time() - start
            print(f"🕒 Processing time: {elapsed:.2f}s")
            return jsonify({"name": best_match, "box": [x, y, w, h], "similarity": float(best_similarity)})
        else:
            print(f"❌ Best match {best_match} with similarity {best_similarity:.4f} below threshold")
            return jsonify({"message": "Person not recognized", "best_match": best_match,
                            "similarity": float(best_similarity)}), 404

    except Exception as e:
        print(f"Error in face recognition: {e}")
        return jsonify({"message": f"Error processing image: {str(e)}"}), 500


@socketio.on('start_recognition')
def start_recognition(data, callback=None):  # Make sure to accept the callback parameter
    global loaded_embeddings
    print(f"Starting recognition with {len(loaded_embeddings)} loaded embeddings")
    # Store session data like selected intakes and courses
    intake = data['intake']
    course = data['course']

    # Call the callback to acknowledge receipt and send back status
    if callback:
        callback({'status': 'success', 'message': 'Recognition started'})

    return {'status': 'success', 'message': 'Recognition started'}


@socketio.on('process_frame')
def process_frame(data):
    print("Received frame for processing")
    try:
        # Decode the image
        img_data = data['image'].split(',')[1]
        img_bytes = base64.b64decode(img_data)
        nparr = np.frombuffer(img_bytes, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

        # Get original dimensions for scaling back bounding boxes if needed
        original_height, original_width = img.shape[:2]

        # Limit processing resolution - resize if too large
        max_width = 240
        if img.shape[1] > max_width:
            scale_factor = max_width / img.shape[1]
            new_height = int(img.shape[0] * scale_factor)
            img = cv2.resize(img, (max_width, new_height))

        # Run face detection
        results = detector.detect_faces(img)
        print(f"Face detection results: {len(results)} faces found")

        # Process only the first face if multiple detected to save processing time
        recognized_faces = []

        for i, face in enumerate(results):
            # Process max 3 faces per frame to avoid overloading
            if i >= 3:
                break

            x, y, w, h = face["box"]

            # Draw bounding box
            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Thinner line

            # Crop face for recognition
            cropped = img[y:y + h, x:x + w]
            try:
                # Get embedding and find match
                emb = DeepFace.represent(cropped, model_name='ArcFace', enforce_detection=False)[0]['embedding']

                # Find best match
                best_match = None
                best_similarity = 0

                for name, saved_emb in loaded_embeddings.items():
                    similarity = cosine_similarity(emb, saved_emb)
                    if similarity > best_similarity:
                        best_similarity = similarity
                        best_match = name

                # If match found with good confidence
                if best_similarity > 0.75:
                    # Draw name above face - simplified text
                    cv2.rectangle(img, (x, y - 20), (x + 100, y), (0, 0, 0), -1)
                    cv2.putText(img, best_match.split('_')[0], (x + 5, y - 5),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

                    # Add to recognized faces
                    recognized_faces.append({
                        'name': best_match,
                        'similarity': float(best_similarity)
                    })
            except Exception as e:
                print(f"Error recognizing face: {e}")

        # Create a smaller image for transmission
        img_small = cv2.resize(img, (240, int(img.shape[0] * 240 / img.shape[1])))

        # Convert processed image back to base64, with higher compression
        _, buffer = cv2.imencode('.jpg', img_small, [cv2.IMWRITE_JPEG_QUALITY, 60])
        processed_image = base64.b64encode(buffer).decode('utf-8')

        # Send processed frame back to client
        socketio.emit('video_frame', {'frame': processed_image})

        # Emit recognition events separately
        for face in recognized_faces:
            socketio.emit('recognition_event', {
                'type': 'recognition',
                'name': face['name'],
                'similarity': face['similarity']
            })

    except Exception as e:
        print(f"Error in process_frame: {e}")
        socketio.emit('error', {'message': str(e)})


@socketio.on('stop_recognition')
def stop_recognition():
    # Any cleanup needed when stopping
    return {'status': 'success', 'message': 'Recognition stopped'}


if __name__ == '__main__':
    # For development with socketio
    socketio.run(app, host='0.0.0.0', port=5000, debug=True)

    # Comment out the regular Flask run
    # app.run(port=5000, threaded=True)















/* TakeAttendenceCamera.css */

.split-container {
  display: flex;
  width: 100%;
  height: 100vh;
  gap: 20px;
  padding: 20px;
  box-sizing: border-box;
}

/* Left side containing camera and controls */
.left-pane {
  display: flex;
  flex-direction: column;
  width: 620px; /* Slightly wider than the camera to account for padding/borders */
  height: 100%;
}

/* Camera container with proper positioning for video and canvas overlay */
.camera_canvas {
  position: relative;
  width: 600px;
  height: 480px;
  margin-bottom: 20px;
  border: 2px solid #ccc;
  border-radius: 4px;
  overflow: hidden; /* Keep canvas and video within bounds */
  background-color: #000;
}

.camera_canvas video {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  object-fit: cover;
}

.camera_canvas canvas {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  z-index: 10; /* Place canvas above video for drawings */
}

/* Control buttons container */
.control-buttons {
  display: flex;
  gap: 10px;
  padding: 10px 0;
}

.control-buttons button {
  flex: 1;
  padding: 12px;
  background-color: #4CAF50;
  color: white;
  border: none;
  border-radius: 4px;
  cursor: pointer;
  font-size: 16px;
  font-weight: bold;
  transition: background-color 0.3s;
}

.control-buttons button:hover {
  background-color: #45a049;
}

.control-buttons button:disabled {
  background-color: #cccccc;
  cursor: not-allowed;
}

.control-buttons button:nth-child(2) {
  background-color: #f44336;
}

.control-buttons button:nth-child(2):hover {
  background-color: #d32f2f;
}

.control-buttons button:nth-child(3) {
  background-color: #2196F3;
}

.control-buttons button:nth-child(3):hover {
  background-color: #0b7dda;
}

/* Right side containing the attendance table */
.right-pane {
  flex: 1;
  display: flex;
  flex-direction: column;
  height: 100%;
  border: 1px solid #ddd;
  border-radius: 4px;
  padding: 15px;
  box-shadow: 0 2px 4px rgba(0,0,0,0.1);
  background-color: #f9f9f9;
  overflow: hidden;
}

.right-pane h2 {
  margin-top: 0;
  margin-bottom: 15px;
  padding-bottom: 10px;
  border-bottom: 2px solid #eee;
  color: #333;
  text-align: center;
}

/* Attendance table styles */
.attendence_table {
  width: 100%;
  border-collapse: collapse;
  overflow-y: auto;
  flex: 1;
}

.attendence_table th {
  background-color: #4CAF50;
  color: white;
  text-align: left;
  padding: 12px;
  position: sticky;
  top: 0;
  z-index: 1;
}

.attendence_table td {
  padding: 10px 12px;
  border-bottom: 1px solid #ddd;
}

.attendence_table tbody tr:nth-child(even) {
  background-color: #f2f2f2;
}

.attendence_table tbody tr:hover {
  background-color: #ddd;
}

/* Responsive adjustments */
@media (max-width: 1200px) {
  .split-container {
    flex-direction: column;
    height: auto;
  }
  
  .left-pane {
    width: 100%;
    max-width: 620px;
    margin: 0 auto;
  }
  
  .right-pane {
    width: 100%;
    max-width: 620px;
    margin: 20px auto 0;
    height: 400px; /* Fixed height for table in mobile view */
  }
}









